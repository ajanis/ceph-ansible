---

telegraf_plugins_extra:
  - name: ceph
    options:
      interval: "1m"
      ceph_user: "client.admin"
      ceph_config: "/etc/ceph/ceph.conf"
      gather_admin_socket_stats: "true"
      gather_cluster_stats: "false"
  - name: bond
    options:
      host_proc: "/proc"
      bond_interfaces:
        - "bond0"
  - name: ipmi_sensor
    options:
      interval: "30s"
      metric_version: 2
  - name: smart
    options:
      use_sudo: "true"
  - name: temp
  - name: sensors

ceph_release_num:
  dumpling: 0.67
  emperor: 0.72
  firefly: 0.80
  giant: 0.87
  hammer: 0.94
  infernalis: 9
  jewel: 10
  kraken: 11
  luminous: 12
  mimic: 13
  nautilus: 14
  dev: 99
fetch_directory: fetch/
cluster: ceph
mon_group_name: mons
osd_group_name: osds
rgw_group_name: rgws
mds_group_name: mdss
nfs_group_name: nfss
rbdmirror_group_name: rbdmirrors
client_group_name: clients
iscsi_gw_group_name: iscsigws
mgr_group_name: mgrs
configure_firewall: False
ceph_conf_local: true
debian_package_dependencies:
  - python-pycurl
ceph_test: false
ntp_service_enabled: true
ntp_daemon_type: ntpd
upgrade_ceph_packages: True
ceph_origin: repository
ceph_repository: community
ceph_mirror: http://download.ceph.com
ceph_stable_key: https://download.ceph.com/keys/release.asc
ceph_stable_release: nautilus
ceph_stable_repo: "{{ ceph_mirror }}/debian-{{ ceph_stable_release }}"
fsid: "{{ cluster_uuid.stdout }}"
generate_fsid: true
ceph_conf_key_directory: /etc/ceph
copy_admin_key: false
ceph_keyring_permissions: '0600'
cephx: false
rbd_client_directories: true # this will create rbd_client_log_path and rbd_client_admin_socket_path directories with proper permissions
rbd_client_directory_owner: root
rbd_client_directory_group: root
rbd_client_directory_mode: 0755
rbd_client_log_path: /var/log/ceph
rbd_client_log_file: "{{ rbd_client_log_path }}/qemu-guest-$pid.log" # must be writable by QEMU and allowed by SELinux or AppArmor
rbd_client_admin_socket_path: /var/run/ceph # must be writable by QEMU and allowed by SELinux or AppArmor
monitor_address_block: 192.168.1.0/24
ip_version: ipv4
mon_use_fqdn: false # if set to true, the MON name used will be the fqdn in the ceph.conf
cephfs: cephfs # name of the ceph filesystem
cephfs_data: cephfs_data # name of the data pool for a given filesystem
cephfs_metadata: cephfs_metadata # name of the metadata pool for a given filesystem
cephfs_pools: []
is_hci: false
non_hci_safety_factor: 0.7
osd_memory_target: 1073741824
journal_size: 5120 # OSD journal size in MB
block_db_size: -1 # block db size in bytes for the ceph-volume lvm batch. -1 means use the default of 'as big as possible'.
public_network: 192.168.1.0/24
cluster_network: "{{ public_network | regex_replace(' ', '') }}"
osd_mkfs_type: xfs
osd_mkfs_options_xfs: -f -i size=2048
osd_mount_options_xfs: noatime,largeio,inode64,swalloc
osd_objectstore: bluestore
mds_use_fqdn: false # if set to true, the MDS name used will be the fqdn in the ceph.conf
mds_max_mds: 3
radosgw_frontend_type: civetweb # For additionnal frontends see: http://docs.ceph.com/docs/mimic/radosgw/frontends/
radosgw_civetweb_port: 8080
radosgw_civetweb_num_threads: 100
radosgw_civetweb_options: "num_threads={{ radosgw_civetweb_num_threads }}"
radosgw_address_block: 192.168.1.0/24
email_address: alan.janis@gmail.com
handler_health_mon_check_retries: 5
handler_health_mon_check_delay: 10
handler_health_osd_check_retries: 40
handler_health_osd_check_delay: 30
handler_health_osd_check: true
handler_health_mds_check_retries: 5
handler_health_mds_check_delay: 10
handler_health_rgw_check_retries: 5
handler_health_rgw_check_delay: 10
handler_health_nfs_check_retries: 5
handler_health_nfs_check_delay: 10
handler_health_rbd_mirror_check_retries: 5
handler_health_rbd_mirror_check_delay: 10
handler_health_mgr_check_retries: 5
handler_health_mgr_check_delay: 10
rgw_multisite: false
ceph_conf_overrides:
  global:
    mon_max_pg_per_osd: 1024
    pool_default_pg_num: 1024
    pool_default_pgp_num: 1024
disable_transparent_hugepage: false
os_tuning_params:
  - { name: fs.file-max, value: 26234859 }
  - { name: vm.zone_reclaim_mode, value: 0 }
  - { name: vm.swappiness, value: 10 }
  - { name: vm.min_free_kbytes, value: "{{ vm_min_free_kbytes }}" }
ceph_tcmalloc_max_total_thread_cache: 0
docker_exec_cmd: []
docker: false
ceph_docker_on_openstack: false
containerized_deployment: false
container_binary: []
rolling_update: false
openstack_config: false
openstack_pools: []
openstack_keys: []
